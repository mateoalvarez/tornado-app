from pyspark.ml import Pipeline, PipelineModel

last_column_name = pipeline_preprocessing_stages[-1].getOutputCol()

preprocessing_pipeline = Pipeline(stages=pipeline_preprocessing_stages)

preprocessing_pipeline_trained = preprocessing_pipeline.\
fit(input_data_df).\
transform(input_data_df).\
withColumnRenamed(last_column_name, "features")

models_pipelines = []

for model_pipeline in pipeline_models_stages:

    models_pipelines.append(model_pipeline.fit(preprocessing_pipeline_trained))
